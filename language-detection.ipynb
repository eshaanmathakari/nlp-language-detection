{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding a Language Detection model in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for Installing necessary package if not installed already\n",
    "#pip install transformers torch datasets pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Roberta Base Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/apple/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your file path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/Users/apple/Desktop/PG/Summer-24/NLP/nlp-language-detection/dataset/language-detection-full-dataset.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "df['Text'] = df['Text'].str.lower().apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Different languages with their respective Labels\n",
    "label_to_language = df[['Label', 'Language']].drop_duplicates().set_index('Label').to_dict()['Language']\n",
    "language_to_label = {v: k for k, v in label_to_language.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting ASCII Language Code to Full Name \n",
    "language_code_to_name = {\n",
    "    \"af\": \"Afrikaans\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"bg\": \"Bulgarian\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"de\": \"German\",\n",
    "    \"el\": \"Greek\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"et\": \"Estonian\",\n",
    "    \"fa\": \"Persian\",\n",
    "    \"fi\": \"Finnish\",\n",
    "    \"fr\": \"French\",\n",
    "    \"gu\": \"Gujarati\",\n",
    "    \"he\": \"Hebrew\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"hr\": \"Croatian\",\n",
    "    \"hu\": \"Hungarian\",\n",
    "    \"id\": \"Indonesian\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"kn\": \"Kannada\",\n",
    "    \"ko\": \"Korean\",\n",
    "    \"lt\": \"Lithuanian\",\n",
    "    \"lv\": \"Latvian\",\n",
    "    \"ml\": \"Malayalam\",\n",
    "    \"mr\": \"Marathi\",\n",
    "    \"ne\": \"Nepali\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"no\": \"Norwegian\",\n",
    "    \"pa\": \"Punjabi\",\n",
    "    \"pl\": \"Polish\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ro\": \"Romanian\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"si\": \"Sinhala\",\n",
    "    \"sk\": \"Slovak\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"sq\": \"Albanian\",\n",
    "    \"sv\": \"Swedish\",\n",
    "    \"sw\": \"Swahili\",\n",
    "    \"ta\": \"Tamil\",\n",
    "    \"te\": \"Telugu\",\n",
    "    \"th\": \"Thai\",\n",
    "    \"tl\": \"Tagalog\",\n",
    "    \"tr\": \"Turkish\",\n",
    "    \"uk\": \"Ukrainian\",\n",
    "    \"ur\": \"Urdu\",\n",
    "    \"vi\": \"Vietnamese\",\n",
    "    \"zh\": \"Chinese\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained language detection model\n",
    "model_name = \"papluca/xlm-roberta-base-language-detection\"\n",
    "language_detection_pipeline = pipeline(\"text-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Function using pretrained model\n",
    "def predict_language(text):\n",
    "    # Preprocess the input text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Predict the language label using the pretrained model\n",
    "    prediction = language_detection_pipeline(text)\n",
    "    # Extract the predicted language code from the model output\n",
    "    predicted_code = prediction[0]['label']\n",
    "    # Convert language code to full language name\n",
    "    language_name = language_code_to_name.get(predicted_code, \"Unknown\")\n",
    "    return language_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the prediction function\n",
    "sample_text = \"Insert Text Here\"\n",
    "predicted_language = predict_language(sample_text)\n",
    "print(f\"The predicted language for the input text is: {predicted_language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/apple/Desktop/PG/Summer-24/NLP/language-detection-full-dataset.csv')\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply data cleaning to the 'Text' column\n",
    "df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = df['cleaned_text']\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb', max_features=50000)),\n",
    "    ('clf', RandomForestClassifier(n_jobs=-1, random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [None, 10, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to predict language for new text\n",
    "def predict_language(text):\n",
    "    cleaned = clean_text(text)\n",
    "    prediction = best_model.predict([cleaned])[0]\n",
    "    language = df[df['Label'] == prediction]['Language'].iloc[0]\n",
    "    return language\n",
    "\n",
    "# Test the model with some example sentences\n",
    "examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Hola, ¿cómo estás?\",\n",
    "    \"Ciao, come stai?\",\n",
    "    \"Hallo, wie geht es dir?\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions for example sentences:\")\n",
    "for example in examples:\n",
    "    predicted_language = predict_language(example)\n",
    "    print(f\"Text: '{example}' - Predicted Language: {predicted_language}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
